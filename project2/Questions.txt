1) Question 1 in the project provides an example where minimax predicts Pacman will do quite poorly, but he actually often wins the game. Briefly explain why this might occur. (There are a number of possible responses to this question; you need only give one.)

It is possible that the ghosts are not acting to minimize the pacman score, rather they're acting more or less randomly.

2) Describe the evaluation function in Question 4 - what features does it include, how do you combine them. This can be copied/pasted from your comments in the code; it's mainly here to make sure that somewhere, you describe your function! 

We have a series of features with given weights that are applied to every state when determining value. These features include the number of food left, the distance to the closest capsule, and the distance to the nearest ghost (positive or negative value based on whether it is scared or not scared). These are weighted with values 50, 1, and 8, respectively. The distance to the nearest not scared ghost is ignored if the Manhattan distance is greater than 6.


3) MCTS: Run your algorithm for 15 games with rollouts set to 75. This may take a little time, but not more than a minute or two. How many games does player 1 win? 

Player 1 won 14/15 games.

4) Repeat (4), except now make MCTS player 2. How many games does player 1 win? (If your answers to (4) and (5) don't make sense to you based on which player should be the smarter player, you probably have an error in your implementation.)

Player 1 only won 1/15 games (with his --rollouts set to 0).

5) Experiment with several values of the UCB exploration constant. Report what values you used and your results for how this affects your MCTS agent's effectiveness. Explain your hypotheses about why you see these results, tying back to what you know about how varying this constant affects the algorithm. Your experimentation should be sufficiently extensive to explore at least some trend in the results.

We tested values of 0.01, .1, .5, 1, 5, 10 and found a general trend that with a small number of rollouts it doesn't really make a difference, but the smaller the ucb constant, the better with a large number of rollouts (i.e. 40). This is because when you don't explore much, you want to explore a lot, but when you are going to explore a lot of nodes anyway, you want to stay focused on the ones that will return the highest utility.

6) Change the UCB exploration constant back to its original value (.5) and experiment with two MCTS players with different numbers of rollouts relative to one another. For example, you might look at an agent with 10 rollouts and an agent with 20 rollouts, and then look at an agent with 20 rollouts versus an agent with 40 rollouts. As in (6) report what values you used and your results. Explain your hypotheses about why you see these results, tying back to what you know about the algorithm. Your experimentation should be sufficiently extensive to explore at least some trend in the results.

In the 10 vs 20 case, there isn't a whole lot of difference, the player with 20 rollouts will win 8-12 games. In the 10 vs 40 case, we observed the 40 rollout player winning 10-15 games. When we tested 20 vs 40, we found that the 40 rollout player won 9-12 games. Finally we tested 10 vs 75, and here the 75 rollout player won 12-15 games consistently. It is expected that against 10 rollouts, more rollouts will do better, as they get a bigger picture. However, when the lesser rollout player has 20 or more rollouts, the returns from getting more rollouts diminish.
